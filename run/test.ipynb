{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from scipy.signal.windows import gaussian\n",
    "from scipy.signal import stft, istft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate=22050\n",
    "n_fft=512\n",
    "root = '../Data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Dataset import AudioMNIST\n",
    "import src.custom_transforms as CT\n",
    "# Test the dataset\n",
    "try:\n",
    "    transforms = [\n",
    "        CT.TrimSilence(5),\n",
    "        CT.TimeStretchFixLength(sample_rate)\n",
    "    ]\n",
    "    dataset = AudioMNIST(target_sample_rate=sample_rate, transform=T.Compose(transforms))\n",
    "    mel_spec, label = dataset[0]\n",
    "    print(\"Success!\")\n",
    "    print(\"Mel spectrogram shape:\", mel_spec.shape)\n",
    "    print(\"Label:\", label)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Value range: [{mel_spec.min():.3f}, {mel_spec.max():.3f}]\")\n",
    "    \n",
    "    # Test for NaN/Inf values\n",
    "    print(\"Contains NaN:\", torch.isnan(mel_spec).any())\n",
    "    print(\"Contains Inf:\", torch.isinf(mel_spec).any())\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"Mean value: {mel_spec.mean():.3f}\")\n",
    "    print(f\"Std value: {mel_spec.std():.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Zxx , _= dataset[15] \n",
    "\n",
    "Zxxx = np.abs(Zxx)\n",
    "Zxxx = np.atleast_3d(Zxxx).transpose(2,0,1)\n",
    "        # convert to decibel\n",
    "Zxxx = librosa.amplitude_to_db(Zxxx, ref = np.max)\n",
    "import matplotlib.pyplot as plt\n",
    "librosa.display.specshow(Zxxx[0])\n",
    "plt.title(\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 173]             608\n",
      "       BatchNorm2d-2         [-1, 32, 128, 173]              64\n",
      "              ReLU-3         [-1, 32, 128, 173]               0\n",
      "         MaxPool2d-4           [-1, 32, 64, 86]               0\n",
      "           Dropout-5           [-1, 32, 64, 86]               0\n",
      "            Conv2d-6           [-1, 64, 64, 86]          18,496\n",
      "       BatchNorm2d-7           [-1, 64, 64, 86]             128\n",
      "              ReLU-8           [-1, 64, 64, 86]               0\n",
      "         MaxPool2d-9           [-1, 64, 32, 43]               0\n",
      "          Dropout-10           [-1, 64, 32, 43]               0\n",
      "           Conv2d-11          [-1, 128, 32, 43]          73,856\n",
      "      BatchNorm2d-12          [-1, 128, 32, 43]             256\n",
      "             ReLU-13          [-1, 128, 32, 43]               0\n",
      "        MaxPool2d-14          [-1, 128, 16, 21]               0\n",
      "          Dropout-15          [-1, 128, 16, 21]               0\n",
      "           Conv2d-16          [-1, 256, 16, 21]         295,168\n",
      "      BatchNorm2d-17          [-1, 256, 16, 21]             512\n",
      "             ReLU-18          [-1, 256, 16, 21]               0\n",
      "        MaxPool2d-19           [-1, 256, 8, 10]               0\n",
      "          Dropout-20           [-1, 256, 8, 10]               0\n",
      "           Linear-21                  [-1, 512]      10,486,272\n",
      "             ReLU-22                  [-1, 512]               0\n",
      "          Dropout-23                  [-1, 512]               0\n",
      "           Linear-24                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 10,880,490\n",
      "Trainable params: 10,880,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.17\n",
      "Forward/backward pass size (MB): 35.29\n",
      "Params size (MB): 41.51\n",
      "Estimated Total Size (MB): 76.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from src.Classifier import LightningAudioClassifier\n",
    "\n",
    "model = LightningAudioClassifier()\n",
    "\n",
    "\n",
    "summary(model, (2, 128, 173),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
